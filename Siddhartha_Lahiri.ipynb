{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe5f41c",
   "metadata": {},
   "source": [
    "# Lending Club Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e6f25306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing core libraries required for the case study\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sea\n",
    "import datetime as dt\n",
    "\n",
    "# Setting max rows settings to 200 to display all the summary data\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f365196",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2b363dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the loan raw dataset\n",
    "loan = pd.read_csv('./.data/loan.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8fc1a7",
   "metadata": {},
   "source": [
    "## DataSet Analysis\n",
    "\n",
    "The data given below contains the information about past loan applicants and whether they ‘defaulted’ or not. The aim is to identify patterns which indicate if a person is likely to default, which may be used for taking actions such as denying the loan, reducing the amount of loan, lending (to risky applicants) at a higher interest rate, etc.\n",
    "\n",
    "- The dataset reflects loans post approval, thus does not represent any information on the rejection criteria process\n",
    "    - Overall objective will be to observe key leading indicaters (driver variables) in the dataset, which contribute to defaulters\n",
    "    - Use the analysis as a the foundation of the hypothesis\n",
    "- The overall loan process is represented by three steps\n",
    "    - Potential borrower requests for loan amount (loan_amnt)\n",
    "    - The approver approves/rejects an amount based on past history/risk (funded_amnt)\n",
    "    - The final amount offered as loan by the investor (funded_amnt_inv)\n",
    "\n",
    "### Leading Attribute\n",
    "- Loan Status - Key Leading Attribute (loan_status). The column has three distinct values\n",
    "    - Fully-Paid - The customer has successfuly paid the loan\n",
    "    - Charged-Off - The customer is \"Charged-Off\" ir has \"Defaulted\"\n",
    "    - Current - These customers, the loan is currently in progress and cannot contribute to conclusive evidence if the customer will default of pay in future\n",
    "        - For the given case study, \"Current\" status rows will be ignored\n",
    "\n",
    "### Important Columns\n",
    "The given columns are leading attributes, or **predictors**. These attributes are available at the time of the loan application and strongly helps in **prediction** of loan pass or rejection. Key attributes *Some of these columns may get dropped due to empty data in the dataset*\n",
    "* **Customer Demographics**\n",
    "  * Annual Income (annual_inc) - Annual income of the customer. Generally higher the income, more chances of loan pass\n",
    "  * Home Ownership (home_ownership) - Wether the customer owns a home or stays rented. Owning a home adds a collateral which increases the chances of loan pass.\n",
    "  * Employment Length (emp_length) - Employment tenure of a customer (this is overall tenure). Higher the tenure, more financial stablity, thus higher chances of loan pass\n",
    "  * Debt to Income (dti) - The percentage of the salary which goes towards paying loan. Lower DTI, higher the chances of a loan pass.\n",
    "  * State (addr_state) - Location of the customer. Can be used to create a generic demographic analysis. There could be higher delinquency or defaulters demographicaly. \n",
    "* **Loan Attributes**\n",
    "  * Loan Ammount (loan_amt) \n",
    "  * Grade (grade)\n",
    "  * Term (term)\n",
    "  * Loan Date (issue_date)\n",
    "  * Purpose of Loan (purpose)\n",
    "  * Verification Status (verification_status)\n",
    "  * Interest Rate (int_rate)\n",
    "  * Installment (installment)\n",
    "  * Public Records (public_rec) - Derogatory Public Records. The value adds to the risk to the loan. Higher the value, lower the success rate.\n",
    "  * Public Records Bankruptcy  (public_rec_bankruptcy) - Number of bankruptcy records publocally available for the customer. Higher the value, lower is the success rate.\n",
    "  \n",
    "\n",
    "### Ignored Columns\n",
    "* The following types of columns will be ignored in the analysis. This is a generic categorization of the columns which will be ignored in our approach and not the full list.\n",
    "   * **Null Columns** - The columns haveing consistent null values across the rows and will be ignored\n",
    "   * **Constant Columns** - The columns having same values across the rows will be ignored as they wont contribute to the analysis\n",
    "   * **Redundant Columns** - The columns which are redundant example the url is one on one derivation of the loan id\n",
    "   * **ID Columns** - The columns which represent some ID of the row data and does not have any corellation with leading attribute(loan_status) will be ignored eg. loan_id, member_id etc.\n",
    "   * **Name and Description Columns** - For the perspective of current case study, the name or description of the company will not contribute to analysis and will be ignored. In future, NLP can be applied to description to gain intent and attributes. For this case study, it will be out of scope.\n",
    "   * **Customer Behaviour Columns** - Columns which describes customer behaviour will not contribute to the analysis. The current analysis is at the time of loan application but the customer behaviour variables generate post the approval of loan applications. Thus these attributes wil not be considered towards the loan approval/rejection process.\n",
    "   * Granular Data - Columns which describe next level of details which may not be required for the analysis. For example grade may be relevant for creating business outcomes and visualizations, sub grade is be very granular and will not be used in the analysis\n",
    "\n",
    "### Ignored Rows and Columns because of missing data\n",
    "*  Columns with high percentage of missing values will be dropped (60% above for this case study)\n",
    "*  Columns with less percentage of missing value will be imputed\n",
    "*  Rows with high percentage of missing values will be removed (60% above for this case study)\n",
    "\n",
    "### Decision Matrix\n",
    "* *Loan Accepted* - Three Scenarios\n",
    "    * *Fully Paid* -  Applicant has fully paid the loan (the principal and the interest rate)\n",
    "    * *Current* - Applicant is in the process of paying the instalments, i.e. the tenure of the loan is not yet completed. These candidates are not labelled as 'defaulted'.\n",
    "    * *Charged-off* - Applicant has not paid the instalments in due time for a long period of time, i.e. he/she has *defaulted* on the loan \n",
    "* *Loan Rejected* - The company had rejected the loan (because the candidate does not meet their requirements etc.). Since the loan was rejected, there is no transactional history of those applicants with the company and so this data is not available with the company (and thus in this dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a39b9",
   "metadata": {},
   "source": [
    "## Approach / Workflow\n",
    "### Checklist\n",
    "- Step 0 - Data Cleaning & Manipulation Checklist\n",
    "\n",
    "### Cleaning & Manipulation\n",
    "- Step1 - Dropping Rows - where loan_status = \"Current\"\n",
    "- Step2 - Dropping Columns\n",
    "- Step3 - Convert the data types\n",
    "- Step 4 - Identify columns with blank values which need to be imputed\n",
    "    - Take a decision to drop or impute (Decision: impute)\n",
    "- Step 5 - Data Conversion of emp_length post treatment of empty rows\n",
    "\n",
    "### Analysis\n",
    "\n",
    "### Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8067b0",
   "metadata": {},
   "source": [
    "## Step 0 - Data Cleaning & Manipulation Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6122eb",
   "metadata": {},
   "source": [
    "### Columns Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "eed4f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                    0\n",
      "member_id                             0\n",
      "loan_amnt                             0\n",
      "funded_amnt                           0\n",
      "funded_amnt_inv                       0\n",
      "term                                  0\n",
      "int_rate                              0\n",
      "installment                           0\n",
      "grade                                 0\n",
      "sub_grade                             0\n",
      "emp_title                          2459\n",
      "emp_length                         1075\n",
      "home_ownership                        0\n",
      "annual_inc                            0\n",
      "verification_status                   0\n",
      "issue_d                               0\n",
      "loan_status                           0\n",
      "pymnt_plan                            0\n",
      "url                                   0\n",
      "desc                              12940\n",
      "purpose                               0\n",
      "title                                11\n",
      "zip_code                              0\n",
      "addr_state                            0\n",
      "dti                                   0\n",
      "delinq_2yrs                           0\n",
      "earliest_cr_line                      0\n",
      "inq_last_6mths                        0\n",
      "mths_since_last_delinq            25682\n",
      "mths_since_last_record            36931\n",
      "open_acc                              0\n",
      "pub_rec                               0\n",
      "revol_bal                             0\n",
      "revol_util                           50\n",
      "total_acc                             0\n",
      "initial_list_status                   0\n",
      "out_prncp                             0\n",
      "out_prncp_inv                         0\n",
      "total_pymnt                           0\n",
      "total_pymnt_inv                       0\n",
      "total_rec_prncp                       0\n",
      "total_rec_int                         0\n",
      "total_rec_late_fee                    0\n",
      "recoveries                            0\n",
      "collection_recovery_fee               0\n",
      "last_pymnt_d                         71\n",
      "last_pymnt_amnt                       0\n",
      "next_pymnt_d                      38577\n",
      "last_credit_pull_d                    2\n",
      "collections_12_mths_ex_med           56\n",
      "mths_since_last_major_derog       39717\n",
      "policy_code                           0\n",
      "application_type                      0\n",
      "annual_inc_joint                  39717\n",
      "dti_joint                         39717\n",
      "verification_status_joint         39717\n",
      "acc_now_delinq                        0\n",
      "tot_coll_amt                      39717\n",
      "tot_cur_bal                       39717\n",
      "open_acc_6m                       39717\n",
      "open_il_6m                        39717\n",
      "open_il_12m                       39717\n",
      "open_il_24m                       39717\n",
      "mths_since_rcnt_il                39717\n",
      "total_bal_il                      39717\n",
      "il_util                           39717\n",
      "open_rv_12m                       39717\n",
      "open_rv_24m                       39717\n",
      "max_bal_bc                        39717\n",
      "all_util                          39717\n",
      "total_rev_hi_lim                  39717\n",
      "inq_fi                            39717\n",
      "total_cu_tl                       39717\n",
      "inq_last_12m                      39717\n",
      "acc_open_past_24mths              39717\n",
      "avg_cur_bal                       39717\n",
      "bc_open_to_buy                    39717\n",
      "bc_util                           39717\n",
      "chargeoff_within_12_mths             56\n",
      "delinq_amnt                           0\n",
      "mo_sin_old_il_acct                39717\n",
      "mo_sin_old_rev_tl_op              39717\n",
      "mo_sin_rcnt_rev_tl_op             39717\n",
      "mo_sin_rcnt_tl                    39717\n",
      "mort_acc                          39717\n",
      "mths_since_recent_bc              39717\n",
      "mths_since_recent_bc_dlq          39717\n",
      "mths_since_recent_inq             39717\n",
      "mths_since_recent_revol_delinq    39717\n",
      "num_accts_ever_120_pd             39717\n",
      "num_actv_bc_tl                    39717\n",
      "num_actv_rev_tl                   39717\n",
      "num_bc_sats                       39717\n",
      "num_bc_tl                         39717\n",
      "num_il_tl                         39717\n",
      "num_op_rev_tl                     39717\n",
      "num_rev_accts                     39717\n",
      "num_rev_tl_bal_gt_0               39717\n",
      "num_sats                          39717\n",
      "num_tl_120dpd_2m                  39717\n",
      "num_tl_30dpd                      39717\n",
      "num_tl_90g_dpd_24m                39717\n",
      "num_tl_op_past_12m                39717\n",
      "pct_tl_nvr_dlq                    39717\n",
      "percent_bc_gt_75                  39717\n",
      "pub_rec_bankruptcies                697\n",
      "tax_liens                            39\n",
      "tot_hi_cred_lim                   39717\n",
      "total_bal_ex_mort                 39717\n",
      "total_bc_limit                    39717\n",
      "total_il_high_credit_limit        39717\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print summary of Nulls, Blanks in the dataset\n",
    "print(loan.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ac393",
   "metadata": {},
   "source": [
    "### Rows Analysis\n",
    "- Summary Rows: No summary rows were there in the dataset\n",
    "- Header & Footer Rows - No header or footer rows in the dataset\n",
    "- Extra Rows - No column number, indicators etc. found in the dataset\n",
    "- Rows where the **loan_status = CURRENT will be dropped** as CURRENT loans are in progress and will not contribute in the decision making of pass or fail of the loan. The rows are dropped before the column analysis as it also cleans up unecessary column related to CURRENT early and columns with NA values can be cleaned in one go\n",
    "\n",
    "\n",
    "### Columns Analysis\n",
    "\n",
    "#### Drop Columns\n",
    "- There are multiple columns with NA values only (next_pymnt_d, mths_since_last_major_derog, annual_inc_joint, dti_joint, verification_status_joint, tot_coll_amt, tot_cur_bal, open_acc_6m, open_il_6m, open_il_12m, open_il_24m, mths_since_rcnt_il, total_bal_il, il_util, open_rv_12m, open_rv_24m, max_bal_bc, all_util, total_rev_hi_lim, inq_fi, total_cu_tl, inq_last_12m, acc_open_past_24mths, avg_cur_bal, bc_open_to_buy, bc_util, mo_sin_old_il_acct, mo_sin_old_rev_tl_op, mo_sin_rcnt_rev_tl_op, mo_sin_rcnt_tl, mort_acc, mths_since_recent_bc, mths_since_recent_bc_dlq, mths_since_recent_inq, mths_since_recent_revol_delinq, num_accts_ever_120_pd, num_actv_bc_tl, num_actv_rev_tl, num_bc_sats, num_bc_tl, num_il_tl, num_op_rev_tl, num_rev_accts, num_rev_tl_bal_gt_0, num_sats, num_tl_120dpd_2m, num_tl_30dpd, num_tl_90g_dpd_24m, num_tl_op_past_12m, pct_tl_nvr_dlq, percent_bc_gt_75, tot_hi_cred_lim, total_bal_ex_mort, total_bc_limit, total_il_high_credit_limit), the **columns will be dropped**. *This is evaluated after dropping rows with loan_status = Current*\n",
    "- There are multiple columns where the values are only zero, the **columns will be dropped**\n",
    "- There are columns where the values are constant. They dont contribute to the analysis, **columns will be dropped**\n",
    "- There are columns where the value is constant but the other values are NA. The column will be considered as constant. **columns will be dropped**\n",
    "- There are columns where more than 65% of data is empty (mths_since_last_delinq, mths_since_last_record) - **columns will be dropped**\n",
    "- Drop columns (id, column_id) as they are index variables and dont contribute to the analysis\n",
    "- Drop columns (url, emp_title, desc, title) as they are discriptive and text and dont contribute to analysis\n",
    "- Drop columns (delinq_2yrs, earliest_cr_line, inq_last_6mths, open_acc, pub_rec, revol_bal, revol_util, total_acc, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, collection_recovery_fee, last_pymnt_d, last_pymnt_amnt, last_credit_pull_d, application_type) as they contribute to the behaviour of the customer. Behaviour of the customer is recorded post approval of loan and not available at the time of loan approval. Thus these variables will not be considered in analysis and thus dropped\n",
    "\n",
    "#### Convert Column Format\n",
    "- (loan_amnt, funded_amnt, funded_amnt_inv) columns are Object and will be converted to float\n",
    "- (int_rate, installment, dti) columns are Object and will be converted to float\n",
    "- strip \"month\" text from term column and convert to integer\n",
    "- Percentage columns are object. Strip \"%\" characters and convert column to float\n",
    "- issue_d column converted to datetime format\n",
    "\n",
    "#### Standardise Values\n",
    "- All currency columns are rounded off to 2 decimal places as currency are limited to cents/paise etc only.\n",
    "\n",
    "#### Convert Column Values\n",
    "- loan_status column converted to boolean Charged Off = False and Fully Paid = True. This converts the column into ordinal values\n",
    "- emp_length converted to integer with following logic - \n",
    "< 1 year: 0,  2 years: 2,  3 years: 3,  7 years: 7,  4 years: 4,  5 years: 5,  1 year: 1,  6 years: 6,  8 years: 8, 9 years: 9,   10+ years: 10\n",
    "\n",
    "#### Added new columns\n",
    "- verification_status_n added. Considering domain knowledge of lending = Verified > Source Verified > Not Verified. verification_status_n correspond to {Verified: 3, Source Verified: 2. Not Verified: 1} for better analysis\n",
    "- issue_y is year extracted from issue_d\n",
    "- issue_m is month extracted from issue_d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d51550",
   "metadata": {},
   "source": [
    "## Step1 - Dropping Rows - where loan_status = \"Current\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9221696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rows where loan_stats=Current are the data where the loan repayment is currently in progress\n",
    "# The loans which are currently in progress will not contribute to decisions \n",
    "# of default or pass as it's difficult to predict the outcome\n",
    "#\n",
    "# Dropping the rwos early as, dropping all Currrent rows introduces NA columns which can be easily dropped\n",
    "rows_before = len(loan)\n",
    "loan = loan[loan['loan_status'] != \"Current\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7d64d848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped =  1140\n",
      "Percentage of rows dropped =  2.87 %\n"
     ]
    }
   ],
   "source": [
    "# Print current data statistics after dropping rows with loan_status \"CURRENT\"\n",
    "rows_after = len(loan)\n",
    "if debug == True:\n",
    "    print(\"Number of rows dropped = \", (rows_before - rows_after))\n",
    "    print(\"Percentage of rows dropped = \", round((rows_before - rows_after)/rows_before*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61611a50",
   "metadata": {},
   "source": [
    "## Step2 - Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fb459ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns which is unique id in nature. They dont contribute to loan analysis\n",
    "loan = loan.drop(['id','member_id'],  axis=1)\n",
    "\n",
    "# Dropping text/description columns which wont contribute to overall analysis\n",
    "# These are names of establishment etc which will not contribute to loan pass or failure\n",
    "# THe URL column is a static link with id as the attribute. Its a redundant column\n",
    "loan = loan.drop(['url', 'emp_title', 'desc', 'title'],  axis=1)\n",
    "\n",
    "# Dropping column sub_grade as the current analysis will limit to Grade only\n",
    "loan = loan.drop(['sub_grade'],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "22c7cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all columns which refer to behavoural data of customer post loan approval \n",
    "# Behaviour data of the customers are captured post the loan approval\n",
    "# The data is not available at the time of loan approval and thus cannot be used for calculations\n",
    "loan = loan.drop(['delinq_2yrs', 'earliest_cr_line', \n",
    "                          'inq_last_6mths', 'open_acc', 'pub_rec', \n",
    "                          'revol_bal', 'revol_util', 'total_acc', \n",
    "                          'out_prncp', 'out_prncp_inv', 'total_pymnt', \n",
    "                          'total_pymnt_inv', 'total_rec_prncp', \n",
    "                          'total_rec_int', 'total_rec_late_fee', 'recoveries', \n",
    "                          'collection_recovery_fee', 'last_pymnt_d', \n",
    "                          'last_pymnt_amnt', 'last_credit_pull_d', \n",
    "                          'application_type'],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3c40ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all values as NA\n",
      " ['next_pymnt_d', 'mths_since_last_major_derog', 'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit']\n"
     ]
    }
   ],
   "source": [
    "# Dropping all columns whose all the values are NA\n",
    "# Print all NA columns for verification\n",
    "print(\"Columns with all values as NA\\n\", loan.columns[loan.isna().all()].tolist())\n",
    "\n",
    "# Dropping all the columns whose all the records are NaN or Null\n",
    "loan = loan.dropna(axis='columns', how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d35350b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all columns with all zero values\n",
    "loan = loan.loc[:, (loan != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "132295b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with constant values with oir without NA\n",
      "pymnt_plan\n",
      "initial_list_status\n",
      "collections_12_mths_ex_med\n",
      "policy_code\n",
      "chargeoff_within_12_mths\n",
      "tax_liens\n"
     ]
    }
   ],
   "source": [
    "# Function to Drop all columns who have constant values (ignoring NA value)\n",
    "# Example most of the columns is 1 and rest is NA, the column will be dropped\n",
    "# If we have 1,2 and NA, the column wont be dropped\n",
    "print(\"Columns with constant values with oir without NA\")\n",
    "def drop_constant_columns(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].nunique(dropna=True) == 1:\n",
    "            print(c)\n",
    "            df = df.drop(c, axis=1)\n",
    "    return df\n",
    "\n",
    "# Drop all constant columns from df1 (definition of constant is constant value across the rows, this ignores Na values)\n",
    "loan = drop_constant_columns(loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "17934b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with more that 65% empty records\n",
      "mths_since_last_delinq\n",
      "mths_since_last_record\n"
     ]
    }
   ],
   "source": [
    "# Function which checks the amount of empty values in a dataframe and \n",
    "# drops the column if the amount of empty values is more than 65%\n",
    "# 60% is the threshhold percentage which decides imputing vs dropping \n",
    "print(\"Columns with more that 65% empty records\")\n",
    "def drop_mostly_empty_columns(df):\n",
    "    total_rows = len(df)\n",
    "    for c in df.columns:\n",
    "        # Drop columns whose mean na values exceed 65%\n",
    "        if df[c].isna().mean().round(2) >= 0.65:\n",
    "            print(c)\n",
    "            df = df.drop(c, axis=1)\n",
    "    return df\n",
    "loan = drop_mostly_empty_columns(loan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367a812",
   "metadata": {},
   "source": [
    "## Step3 - Convert the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "56a612e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the columns loan_amnt and funded_amnt as flot64\n",
    "loan = loan.astype({'loan_amnt':'float','funded_amnt':'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9f049912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the term column into an integer from a string\n",
    "loan['term'] = loan['term'].apply(lambda x : int(x[:-7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9c61fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert int_rate to  float by removing the \"%\" character\n",
    "loan['int_rate'] = loan['int_rate'].apply(lambda x : float(x[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c7fcc0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounding columns to 2 decimal places\n",
      "loan_amnt\n",
      "funded_amnt\n",
      "funded_amnt_inv\n",
      "int_rate\n",
      "dti\n"
     ]
    }
   ],
   "source": [
    "# Round off the values of key float fields to 2 decimal place\n",
    "# all int_rate and dti already limited to 2 edcimal\n",
    "print(\"Rounding columns to 2 decimal places\")\n",
    "for c in ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'dti']:\n",
    "    if debug == True:\n",
    "        print(c)\n",
    "    loan[c] = loan[c].apply(lambda x: round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "17bba559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the loan_status to boolean column. \"Fully-Paid is True and Charged Off is False\"\n",
    "# Added a function instead of lambda because, if this is accidentally re-run on a boolean column, the logic broke\n",
    "# Now it will only convert to boolean if the column is a string and has the two specific values\n",
    "def convert_loan_status_to_boolean(x):\n",
    "    if x == \"Fully Paid\":\n",
    "        return True\n",
    "    elif x == \"Charged Off\":\n",
    "        return False\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "loan['loan_status'] = loan['loan_status'].apply(lambda x: convert_loan_status_to_boolean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ad26d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the column issue_d from string object to DateTime\n",
    "loan['issue_d'] = pd.to_datetime(loan['issue_d'], format='%b-%y')\n",
    "\n",
    "# Adding additional column for Year and Month for analysis extrating Year and Month from issue_d\n",
    "loan['issue_y'] = pd.DatetimeIndex(loan['issue_d']).year\n",
    "loan['issue_m'] = pd.DatetimeIndex(loan['issue_d']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e4515ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    16694\n",
      "3    12206\n",
      "2     9677\n",
      "Name: verification_status_nominal, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Converting the verification_status as Ordinal value with the following logic\n",
    "# Verified - The documents are verified first hand with specific documents like Bank Statements, Identity Documents etc.\n",
    "# Source Verified - The customer has been verified by third party source\n",
    "# Not Verified - The customer has not been verified\n",
    "# Considering domain knowledge of lending = Verified > Source Verified > Not Verified\n",
    "# Replacing the nominal values with ordinal values {Verified: 3, Source Verified: 2. Not Verified: 1}\n",
    "# Higher number given to more trusted source\n",
    "\n",
    "loan['verification_status_nominal'] = loan['verification_status'].replace({'Verified': 3,\n",
    "                                                                                   'Source Verified': 2,\n",
    "                                                                                   'Not Verified': 1})\n",
    "\n",
    "print(loan['verification_status_nominal'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02694d24",
   "metadata": {},
   "source": [
    "## Step 4 - Identify columns with blank values which need to be imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "72ba1219",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp_length 2.68 %\n",
      "pub_rec_bankruptcies 1.81 %\n"
     ]
    }
   ],
   "source": [
    "# Identify columns who have blank values and what percentage of total values are there blanks. \n",
    "# These values may need to be imputed\n",
    "for c in loan.columns[loan.isna().any()].tolist():\n",
    "    print(c, round(len(loan[loan[c].isna()]) / len(loan) * 100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a351d",
   "metadata": {},
   "source": [
    "#### Two approaches to analysis\n",
    "* **Option 1** - Drop the rows with blank values\n",
    "* **Option 2** - Impute the values with a median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bb2d270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two approaches to analysis\n",
    "# - impute = True - Will replace empty rows with median values\n",
    "# - impute = False - Will drop rows with empty values\n",
    "\n",
    "## Approach taken Option 2\n",
    "impute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "65f9b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current dimensions of the dataframe\n",
    "rows_before = len(loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a9d7a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this approach of taking both scenarios, we will evaluate the variation\n",
    "# If there is not much variation, we will take the drop approach as it will be more accurate as compared to imputing\n",
    "if impute == False:\n",
    "    # Drop rows with empty values in this scenario\n",
    "    # Since the percent of rows is very small, dropping the rows instead of imputing them\n",
    "    loan = loan[loan['emp_length'].notna()]\n",
    "    loan = loan[loan['pub_rec_bankruptcies'].notna()]\n",
    "else:\n",
    "    # Impute values with empty column values\n",
    "    # Since emp_length is a string, using mode to fill the empty values\n",
    "    loan['emp_length'].fillna(loan['emp_length'].mode()[0], inplace=True)\n",
    "    loan['pub_rec_bankruptcies'].fillna(loan['pub_rec_bankruptcies'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "14eb71e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped = , 0\n",
      "Percentage of rows dropped =  0.0 %\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the dataframe after dropping rows\n",
    "rows_after = len(loan)\n",
    "if debug == True:\n",
    "    print(\"Number of rows dropped = ,\", (rows_before - rows_after))\n",
    "    print(\"Percentage of rows dropped = \", round((rows_before - rows_after)/rows_before*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4c702",
   "metadata": {},
   "source": [
    "## Step 5 - Data Conversion of emp_length post treatment of empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8e84efc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10    9521\n",
      "0     4508\n",
      "2     4291\n",
      "3     4012\n",
      "4     3342\n",
      "5     3194\n",
      "1     3169\n",
      "6     2168\n",
      "7     1711\n",
      "8     1435\n",
      "9     1226\n",
      "Name: emp_length, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Converting emp_length to integer values\n",
    "# Converting emp_length as numerical data to create more effective statistical analysis as compared to nominal values\n",
    "loan['emp_length'] = loan['emp_length'].replace({'< 1 year': 0, '2 years': 2, '3 years': 3, \n",
    "                                                         '7 years': 7, '4 years': 4, '5 years': 5, \n",
    "                                                         '1 year': 1, '6 years': 6, '8 years': 8, \n",
    "                                                         '9 years': 9,  '10+ years': 10})\n",
    "\n",
    "print(loan['emp_length'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1cee0e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 38577 entries, 0 to 39716\n",
      "Data columns (total 21 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   loan_amnt                    38577 non-null  float64       \n",
      " 1   funded_amnt                  38577 non-null  float64       \n",
      " 2   funded_amnt_inv              38577 non-null  float64       \n",
      " 3   term                         38577 non-null  int64         \n",
      " 4   int_rate                     38577 non-null  float64       \n",
      " 5   installment                  38577 non-null  float64       \n",
      " 6   grade                        38577 non-null  object        \n",
      " 7   emp_length                   38577 non-null  int64         \n",
      " 8   home_ownership               38577 non-null  object        \n",
      " 9   annual_inc                   38577 non-null  float64       \n",
      " 10  verification_status          38577 non-null  object        \n",
      " 11  issue_d                      38577 non-null  datetime64[ns]\n",
      " 12  loan_status                  38577 non-null  bool          \n",
      " 13  purpose                      38577 non-null  object        \n",
      " 14  zip_code                     38577 non-null  object        \n",
      " 15  addr_state                   38577 non-null  object        \n",
      " 16  dti                          38577 non-null  float64       \n",
      " 17  pub_rec_bankruptcies         38577 non-null  float64       \n",
      " 18  issue_y                      38577 non-null  int64         \n",
      " 19  issue_m                      38577 non-null  int64         \n",
      " 20  verification_status_nominal  38577 non-null  int64         \n",
      "dtypes: bool(1), datetime64[ns](1), float64(8), int64(5), object(6)\n",
      "memory usage: 6.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Printing column info to analyse missing values, empty values in a column\n",
    "print(loan.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0a90149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a snapshot for manual\n",
    "if debug == True:\n",
    "    loan.to_csv('./.data/snapshot.clean.loan.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
